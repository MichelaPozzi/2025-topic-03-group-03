---
title: "Data Reduction"
output: html_document
date: "2025-05-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
MS_Table_norm = read.table("Datensätze/MS_Table.norm.csv", header=TRUE, row.names=1, sep = ",")
```

# PCA
```{r}
# PCA with original values
# only Carl's code

pca = prcomp(MS_Table, center = TRUE, scale = TRUE)
summary(pca)
pca$sdev
variance = (pca$sdev)^2
prop.variance = variance / sum(variance)
names(prop.variance) = 1:length(prop.variance)
barplot(prop.variance[1:20], ylab = 'Proportion of variance') # we only plot the first 20 PCs

plot(pca$x[,1], pca$x[,2], 
     col = 'black', pch = 19,
     xlab = 'PC1', ylab = 'PC2')
```

```{r}
# PCA mit normierten Werten
# nur Carls Code

pca = prcomp(MS_Table_norm, center = TRUE, scale = TRUE)
summary(pca)
pca$sdev
variance = (pca$sdev)^2
prop.variance = variance/sum(variance)
names(prop.variance) = 1:length(prop.variance)
barplot(prop.variance[1:20],ylab='Proportion of variance') # we only plot the first 20 PCs

plot(pca$x[,1], pca$x[,2], 
     col= 'black', pch=19,
     xlab='PC1',ylab='PC2')
```

#Splitting the MS_Table_norm table
```{r}
# Number of fractions
n_fractions <- 25

# Prepare column indices
control_idx <- c()
rnase_idx <- c()

for (f in 0:(n_fractions - 1)) {
  start_col <- 6 * f
  # Control: columns 1-3 of each fraction
  control_idx <- c(control_idx, start_col + 1:3)
  # RNAse: columns 4-6 of each fraction
  rnase_idx <- c(rnase_idx, start_col + 4:6)
}

# Extract tables
MS_Control <- MS_Table_norm[, control_idx]
MS_RNase <- MS_Table_norm[, rnase_idx]
```


#Title: Installing the mentioneed libraries 
#Subtitle: The required packages for clustering analysis and visualization were installed 
#Name: Hasset Gessese
# Implemented AI tools: 
#Remarks: 
   - factoextra: functions to visualize clustering results, including Elbow and Silhouette methods
   - cluster: provides silhouette calculation and clustering algorithms
   - pheatmap: used to create heatmaps for data visualization
   - RColorBrewer: provides aesthetically pleasing color palettes for the plots => those were adjusted later on to the color palette of the poster (blues tones)

```{r install_all_needed, eval=FALSE}
install.packages(c("factoextra", "cluster", "pheatmap", "RColorBrewer"))
# For Elbow and Silhouette methods => library(factoextra)
# For silhouette plots => library(cluster)
# For heatmaps => library(pheatmap)
# For color palettes in plots => library(RColorBrewer)
```

#Title: K-Means Clustering Code (in the colour palette of the poster)
#Subtitle: PCA-Based K-Means Clustering for Normalization Assessment
#Name: Hasset Gessese
# Implemented AI tools: Written based on Plenum Code from our Biological Data Analysis class and asjusted through ChatGPT  (e.g. addition of Silhouette Width values etc.)
#Remarks: 
   - We used this K-means clustering  as a quality control check after normalization to ensure that the data structure remained interpretable and consistent (shows seperability and stability). 
   - Because the optimal k (k=6) from this approach did not align with our modeling results, we did not use these clusters for biological conclusions 
   - Instead we used these results  as a tool to assess the success of normalization.
   
```{r kmeans_pca_clustering, message=FALSE, warning=FALSE}

# Perform Principal Component Analysis (PCA) on the normalized data
pca <- prcomp(MS_Table_norm, center = TRUE, scale. = TRUE)

# Calculate the proportion of variance explained by each principal component
prop.variance <- (pca$sdev)^2 / sum(pca$sdev^2)
pc1_var <- round(prop.variance[1] * 100, 1)  # Variance % explained by PC1
pc2_var <- round(prop.variance[2] * 100, 1)  # Variance % explained by PC2

# Visualize the variance proportions for the top 10 PCs with a barplot
barplot(prop.variance[1:10], main = "Variance Proportions (Top 10 PCs)", ylab = "Proportion")

# Select the first 5 principal components as features for clustering
pca_data <- pca$x[, 1:5]

# Define a custom color palette (poster colors) for cluster visualization
cluster_colors <- c(
  "#7E51A4", "#504DA4", "#485571",
  "#7396AD", "#DBCEE6", "#A9B3B5"
)

# Load required libraries for clustering visualization and analysis
library(factoextra)  # For cluster visualization and determining cluster number
library(cluster)     # For silhouette analysis

# Use the Elbow method to estimate the optimal number of clusters (k)
elbow_plot <- fviz_nbclust(pca_data, kmeans, method = "wss") +
  ggtitle("Elbow Method – Optimal Number of Clusters") +
  geom_vline(xintercept = 6, linetype = 2, color = "blue")  # Mark k=6 for reference
print(elbow_plot)

# Use the Silhouette method to validate the optimal cluster number
silhouette_plot <- fviz_nbclust(pca_data, kmeans, method = "silhouette") +
  ggtitle("Silhouette Method – Optimal Number of Clusters")
print(silhouette_plot)

# Run K-means clustering with k=6 clusters and multiple random starts for stability
set.seed(123)  # Ensure reproducibility
km <- kmeans(pca_data, centers = 6, nstart = 25)
cluster_assignment <- km$cluster  # Extract cluster membership

# Plot the first two PCs with points colored by cluster assignment
plot(
  pca$x[, 1], pca$x[, 2],
  col = cluster_colors[cluster_assignment],
  pch = 19,
  xlab = paste0("PC1 (", pc1_var, "% Variance)"),
  ylab = paste0("PC2 (", pc2_var, "% Variance)"),
  main = "K-Means Clustering on PCA (k=6)"
)

# Draw convex hulls around points of each cluster to visually group them
library(grDevices)  # For polygon drawing
for (k in 1:max(cluster_assignment)) {
  cluster_points <- pca$x[cluster_assignment == k, 1:2]
  if (nrow(cluster_points) >= 3) {  # Need at least 3 points to form a polygon
    hull_indices <- chull(cluster_points)          # Indices of convex hull points
    hull_indices <- c(hull_indices, hull_indices[1])  # Close the polygon
    polygon(
      cluster_points[hull_indices, ],
      border = cluster_colors[k],
      col = adjustcolor(cluster_colors[k], alpha.f = 0.4),  # Semi-transparent fill
      lwd = 2
    )
  }
}

# Prepare for silhouette analysis to assess cluster quality
n_obs <- nrow(pca_data)
sample_threshold <- 1000

# If dataset is large, use a random sample of 500 points for efficiency
if (n_obs > sample_threshold) {
  set.seed(42)
  sample_idx <- sample(1:n_obs, 500)
  pca_sample <- pca_data[sample_idx, ]
  cluster_sample <- cluster_assignment[sample_idx]
  dist_sample <- dist(pca_sample)  # Compute distance matrix on the sample
  sil <- silhouette(cluster_sample, dist_sample)
  message("The silhouette plot is based on a sample of 500 observations")
} else {
  # Otherwise, compute silhouette on full dataset
  dist_matrix <- dist(pca_data)
  sil <- silhouette(cluster_assignment, dist_matrix)
}

library(factoextra)  # For silhouette visualization

# Calculate average silhouette width overall and per cluster
avg_sil_width <- mean(sil[, "sil_width"])
avg_sil_per_cluster <- tapply(sil[, "sil_width"], sil[, "cluster"], mean)

# Prepare annotation text showing average silhouette widths
label_text <- paste0("Average Silhouette Width: ", round(avg_sil_width, 3), "\n",
                     paste0("Cluster ", names(avg_sil_per_cluster), ": ",
                            round(avg_sil_per_cluster, 3),
                            collapse = "\n"))

# Generate silhouette plot with cluster colors and annotated averages
sil_plot <- fviz_silhouette(sil, palette = cluster_colors) +
  ggtitle("Silhouette Plot for K-Means Clustering") +
  theme(plot.title = element_text(hjust = 0.5))

# Add annotation text to the plot
sil_plot +
  annotate("text",
           x = Inf, y = Inf,
           label = label_text,
           hjust = 1.05, vjust = 1.05,
           size = 3,
           fontface = "italic")

# Export the cluster assignments as a CSV file for downstream analysis
cluster_df <- data.frame(Protein = rownames(MS_Table_norm),
                         Cluster = cluster_assignment)

write.csv(cluster_df, "Cluster_Assignment.csv", row.names = FALSE)
```

#The complete Kmean-Clustering Code (not in the poster colours)
#Title: K-Means Clustering Code (in the colour palette of the poster)
#Subtitle: PCA-Based K-Means Clustering for Normalization Assessment
#Name: Hasset Gessese
# Implemented AI tools:  Written based on Plenum Code from our Biological Data Analysis class and asjusted through ChatGPT  (e.g. addition of Silhouette Width values etc.)
#Remarks: 
   - We used this K-means clustering  as a quality control check after normalization to ensure that the data structure remained interpretable and consistent (shows seperability and stability). 
   - Because the optimal k (k=6) from this approach did not align with our modeling results, we did not use these clusters for biological conclusions 
   - Instead we used these results  as a tool to assess the success of normalization.
```{r kmeans_pca_clustering, message=FALSE, warning=FALSE}
# PCA on normalized table
pca <- prcomp(MS_Table_norm, center = TRUE, scale. = TRUE)

# Display variance proportions (optional)
prop.variance <- (pca$sdev)^2 / sum(pca$sdev^2)
barplot(prop.variance[1:10], main = "Variance Proportions (Top 10 PCs)", ylab = "Proportion")

# Select the first 5 principal components for clustering
pca_data <- pca$x[, 1:5]

# Load required packages
library(factoextra)
library(cluster)
library(RColorBrewer)
col <- brewer.pal(9, 'Set1')

# Elbow method
elbow_plot <- fviz_nbclust(pca_data, kmeans, method = "wss") +
  ggtitle("Elbow Method – Optimal Number of Clusters") +
  geom_vline(xintercept = 5, linetype = 2, color = "blue")
print(elbow_plot)

# Silhouette method
fviz_nbclust(pca_data, kmeans, method = "silhouette") +
  ggtitle("Silhouette Method – Optimal Number of Clusters")

# Set seed for reproducibility
set.seed(123)

# K-Means clustering with k = 6 (adjust if needed)
km <- kmeans(pca_data, centers = 6, nstart = 25)
cluster_assignment <- km$cluster

# Plot the first two PCs colored by clusters
plot(
  pca$x[, 1], pca$x[, 2],
  col = col[cluster_assignment],
  pch = 19,
  xlab = "PC1", ylab = "PC2",
  main = "K-Means Clustering on PCA (k=6)"
)

# Draw convex hulls for each cluster
library(grDevices)
for (k in 1:max(cluster_assignment)) {
  cluster_points <- pca$x[cluster_assignment == k, 1:2]
  if (nrow(cluster_points) >= 3) {
    hull_indices <- chull(cluster_points)
    hull_indices <- c(hull_indices, hull_indices[1])
    polygon(
      cluster_points[hull_indices, ],
      border = col[k],
      col = adjustcolor(col[k], alpha.f = 0.4),
      lwd = 2
    )
  }
}

# Add silhouette plot with average silhouette widths ---

# Check if dataset is too large (> 1000 observations)
n_obs <- nrow(pca_data)
sample_threshold <- 1000

if (n_obs > sample_threshold) {
  set.seed(42)
  sample_idx <- sample(1:n_obs, 500)
  pca_sample <- pca_data[sample_idx, ]
  cluster_sample <- cluster_assignment[sample_idx]
  dist_sample <- dist(pca_sample)
  sil <- silhouette(cluster_sample, dist_sample)
  message("The silhouette plot is now based on a sample of 500 observations")
} else {
  dist_matrix <- dist(pca_data)
  sil <- silhouette(cluster_assignment, dist_matrix)
}

# Calculate average silhouette widths
avg_sil_width <- mean(sil[, "sil_width"])
avg_sil_per_cluster <- tapply(sil[, "sil_width"], sil[, "cluster"], mean)

# Create annotation text
label_text <- paste0(
  "Average Silhouette Width: ", round(avg_sil_width, 3), "\n",
  paste0(
    "Cluster ", names(avg_sil_per_cluster), ": ",
    round(avg_sil_per_cluster, 3),
    collapse = "\n"
  )
)

# Create annotated silhouette plot
sil_plot <- fviz_silhouette(sil, palette = col) +
  ggtitle("Silhouette Plot for K-Means Clustering") +
  theme(plot.title = element_text(hjust = 0.5))

sil_plot +
  annotate(
    "text",
    x = Inf, y = Inf,
    label = label_text,
    hjust = 1.05, vjust = 1.05,
    size = 3,
    fontface = "italic"
  )

## Export cluster assignment as CSV
cluster_df <- data.frame(
  Protein = rownames(MS_Table_norm),
  Cluster = cluster_assignment
)
write.csv(cluster_df, "Cluster_Assignment.csv", row.names = FALSE)
```



========================================================================================================================
##Plotvisualisierungen für die Data Exploration 
# Heatmap für Replikatenvergleich  
```{r heatmap_replikate}

print(dim(MS_Table_norm))
print(colnames(MS_Table_norm))


# Pakete laden
library(pheatmap)
library(RColorBrewer)

# Definiere die Replikatspalten (aus deinem Clean-Up bekannt)
replikate <- c("Fraction1_Ctrl_Rep1", "Fraction1_Ctrl_Rep2", "Fraction1_Ctrl_Rep3",
               "Fraction1_RNase_Rep1", "Fraction1_RNase_Rep2", "Fraction1_RNase_Rep3")

# Prüfen, welche Spalten fehlen => nur zur Fehlerfindung => wird dannach rausgenommen 
missing <- replikate[!replikate %in% colnames(MS_Table_norm)]
print(missing)

# Subset aus der normierten Matrix
data_for_heatmap <- MS_Table_norm[, replikate]

# Transponieren: Zeilen = Replikate, Spalten = Proteine
data_for_heatmap_t <- t(data_for_heatmap)

# Farben für die Heatmap
heat_colors <- colorRampPalette(brewer.pal(9, "YlGnBu"))(100)

# Heatmap zeichnen
pheatmap(data_for_heatmap_t,
         scale = "row",  # z-Standardisierung der Replikate
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         clustering_method = "complete",
         color = heat_colors,
         main = "Heatmap: Vergleich der Replikate (MS_Table_norm)")
```


# Heatmap Proteinverteilung RNAse und Kontrolle im Vergleich
```{r heatmap_rnase_kontrolle}
# Libraries
library(pheatmap)
library(RColorBrewer)

# Spaltennamen (Proben)
gruppen <- c("Kontrolle", "Kontrolle", "Kontrolle", "RNAse", "RNAse", "RNAse")
names(gruppen) <- c("Fraction1_Ctrl_Rep1", "Fraction1_Ctrl_Rep2", "Fraction1_Ctrl_Rep3",
                    "Fraction1_RNase_Rep1", "Fraction1_RNase_Rep2", "Fraction1_RNase_Rep3")

# Annotation für die Probenfarben
annotation_col <- data.frame(Gruppe = gruppen)
rownames(annotation_col) <- names(gruppen)

# Farben definieren
group_colors <- list(Gruppe = c(Kontrolle = "lightblue", RNAse = "salmon"))

# Optional: Top 50 variabelste Proteine auswählen
top_var <- apply(MS_Table_norm, 1, var)
top50 <- MS_Table_norm[order(top_var, decreasing = TRUE)[1:50], ]

# Heatmap zeichnen
pheatmap(top50,
         scale = "row",
         annotation_col = annotation_col,
         annotation_colors = group_colors,
         clustering_distance_cols = "euclidean",
         clustering_method = "complete",
         color = colorRampPalette(brewer.pal(9, "RdBu"))(100),
         main = "Proteinverteilung: RNAse vs. Kontrolle")
```

```{r}
df_ergebnisse
```




===========================================================================================================================================================================================================================================================================================================================================================================
==> Vergleich mit Shift_Score ==> KANN RAUS ODER? 

```{r}
# PCA auf normierter Tabelle
pca <- prcomp(MS_Table_norm, center = TRUE, scale. = TRUE)

# Varianz-Anteile anzeigen (optional)
prop.variance <- (pca$sdev)^2 / sum(pca$sdev^2)
barplot(prop.variance[1:10], main = "Varianzanteile (Top 10 PCs)", ylab = "Proportion")

# Auswahl der ersten 5 Hauptkomponenten für Clustering
pca_data <- pca$x[, 1:5]

# Benötigte Pakete laden
library(factoextra)
library(cluster)
library(RColorBrewer)
library(ggplot2)

col <- brewer.pal(9, 'Set1')

# Elbow-Methode mit manueller Knickpunkt-Linie
elbow_plot <- fviz_nbclust(pca_data, kmeans, method = "wss") +
  ggtitle("Elbow-Methode – optimale Clusteranzahl") +
  geom_vline(xintercept = 5, linetype = 2, color = "blue")

print(elbow_plot)

# Silhouette-Methode
fviz_nbclust(pca_data, kmeans, method = "silhouette") +
  ggtitle("Silhouette-Methode – optimale Clusteranzahl")

# Setze Seed für Reproduzierbarkeit
set.seed(123)

# K-Means Clustering mit k=6
km <- kmeans(pca_data, centers = 6, nstart = 25)

# Cluster-Zuordnung speichern
cluster_assignment <- km$cluster

# Plot der ersten zwei PCs mit Clusterfarben
plot(pca$x[,1], pca$x[,2],
     col = col[cluster_assignment],
     pch = 19,
     xlab = "PC1", ylab = "PC2",
     main = "K-Means Clustering auf PCA (k=6)")

# Silhouette-Plot
n_obs <- nrow(pca_data)
sample_threshold <- 1000

if (n_obs > sample_threshold) {
  set.seed(42)
  sample_idx <- sample(1:n_obs, 500)
  pca_sample <- pca_data[sample_idx, ]
  cluster_sample <- cluster_assignment[sample_idx]
  dist_sample <- dist(pca_sample)
  sil <- silhouette(cluster_sample, dist_sample)
  message("Der Silhouette-Plot basiert jetzt auf einer Stichprobe von 500 Beobachtungen")
} else {
  dist_matrix <- dist(pca_data)
  sil <- silhouette(cluster_assignment, dist_matrix)
}

fviz_silhouette(sil) +
  ggtitle("Silhouette-Plot für K-Means Clustering")

# Export der Cluster-Zuordnung als CSV
cluster_df <- data.frame(Protein = rownames(MS_Table_norm),
                         Cluster = cluster_assignment)

write.csv(cluster_df, "Cluster_Zuordnung.csv", row.names = FALSE)


####################
# HIER: Plot mit Shift-Kategorien
####################

# Shift-Kategorien aus df_ergebnisse holen
# Angenommen df_ergebnisse hat:
# - Spalte 1: Proteinname
# - Spalte 14: Shift-Kategorie
shift_df <- data.frame(
  Protein = df_ergebnisse[[1]],
  Shift_Kategorie = df_ergebnisse[[14]]
)

# PCA-Scores (PC1 & PC2) holen und mit Cluster kombinieren
pca_scores <- as.data.frame(pca$x[, 1:2])
pca_scores$Protein <- rownames(pca$x)
pca_scores$Cluster <- factor(cluster_assignment)

# Merge mit Shift-Kategorien
plot_df <- merge(pca_scores, shift_df, by = "Protein")


# Farben für Shift-Kategorien definieren
shift_colors <- c("kein_Shift" = "violet",
                  "moderater_right_shift" = "#E69F00",
                  "starker_right_shift" = "#D55E00",
                  "starker_left_shift" = "#0072B2",
                  "moderater_left_shift" = "#56B4E9")

# Plot: Cluster unverändert, Farbe = Shift-Kategorie
ggplot(plot_df, aes(x = PC1, y = PC2, color = Shift_Kategorie)) +
  geom_point(size = 2, alpha = 0.8) +
  scale_color_manual(values = shift_colors) +
  labs(title = "K-Means Clustering auf PCA (k=6) – eingefärbt nach Shift-Kategorie",
       subtitle = "Cluster unverändert; Farbe zeigt Shift-Kategorie",
       x = "PC1", y = "PC2", color = "Shift-Kategorie") +
  theme_minimal()

```
==> WAS WAR DAS - KANN DAS RAUS???
```{r}
# Dein PCA-Objekt
pca = prcomp(MS_Table_norm, center = TRUE, scale. = TRUE)

# PCA-Scores (Koordinaten der Proteine auf den PCs)
scores = pca$x

# Die Shift-Kategorien aus der Ergebnis-Tabelle holen
shift_kategorie = df_ergebnisse[,14]  # Spalte 14

# Prüfen, dass die Reihenfolge der Kategorien zu den PCA-Scores passt
# Wenn MS_Table_norm und df_ergebnisse dieselben Zeilenreihenfolgen haben, ist das ok.
# Falls nicht, musst du sicherstellen, dass sie aufeinander gematcht sind!

# Kategorien als Faktor (optional mit Reihenfolge)
shift_kategorie = factor(shift_kategorie, levels = c("kein_shift",
                                                     "moderater_left_shift",
                                                     "starker_left_shift",
                                                     "moderater_right_shift",
                                                     "starker_right_shift"))

# Farben für die Kategorien definieren
farben = c("kein_shift"="grey50",
           "moderater_left_shift"="orange",
           "starker_left_shift"="red",
           "moderater_right_shift"="skyblue",
           "starker_right_shift"="blue")

# Layout: 1 Zeile, 2 Spalten
layout(matrix(c(1,2), 1, 2), widths=c(4,1))  # 4 Teile Plot, 1 Teil Legende

# PCA-Plot
par(mar=c(5,4,4,1))  # normaler Rand, rechts kleiner (weil Legende eigene Spalte hat)
plot(scores[,1], scores[,2],
     col=farben[shift_kategorie],
     pch=19,
     xlab="PC1", ylab="PC2",
     main="PCA: Färbung nach Shift-Kategorie")

# Legende in eigenem Feld
par(mar=c(0,0,0,0))  # keine Ränder
plot.new()           # leeres Plotfenster
legend("center", legend=levels(shift_kategorie),
       col=farben, pch=19, title="Shift-Kategorie")
```